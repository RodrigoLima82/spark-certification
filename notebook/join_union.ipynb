{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN() + UNION()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode, col, isnull\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-join-union\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Table\n",
    "empData = [(1,\"Smith\",10), (2,\"Rose\",20),(3,\"Williams\",10), (4,\"Jones\",30)]\n",
    "\n",
    "empColumns = [\"emp_id\",\"name\",\"emp_dept_id\"]\n",
    "\n",
    "empDF = spark.createDataFrame(empData,empColumns)\n",
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dept Table\n",
    "deptData = [(\"Finance\",10), (\"Marketing\",20), (\"Sales\",30),(\"IT\",40)]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF=spark.createDataFrame(deptData,deptColumns)  \n",
    "deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address Table\n",
    "addData=[(1,\"1523 Main St\",\"SFO\",\"CA\"),\n",
    "         (2,\"3453 Orange St\",\"SFO\",\"NY\"),\n",
    "         (3,\"34 Warner St\",\"Jersey\",\"NJ\"),\n",
    "         (4,\"221 Cavalier St\",\"Newark\",\"DE\"),\n",
    "         (5,\"789 Walnut St\",\"Sandiago\",\"CA\")]\n",
    "\n",
    "addColumns = [\"emp_id\", \"addline1\", \"city\", \"state\"]\n",
    "\n",
    "addDF = spark.createDataFrame(addData,addColumns)\n",
    "addDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Join Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join(right, joinExprs, joinType)\n",
    "# join(right)\n",
    "\n",
    "empDF.join(addDF, empDF[\"emp_id\"] == addDF[\"emp_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate Columns After Join\n",
    "empDF.join(addDF,[\"emp_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join Multiple DataFrames by chaining\n",
    "empDF.join(addDF,[\"emp_id\"]) \\\n",
    "     .join(deptDF,empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Where for Join Condition\n",
    "empDF.join(deptDF).where(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "    .join(addDF).where(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Filter for Join Condition\n",
    "empDF.join(deptDF).filter(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "    .join(addDF).filter(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL\n",
    "empDF.createOrReplaceTempView(\"emp\")\n",
    "deptDF.createOrReplaceTempView(\"dept\")\n",
    "addDF.createOrReplaceTempView(\"add\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "             select * \n",
    "               from emp e, dept d, add a\n",
    "              where e.emp_dept_id == d.dept_id \n",
    "                and e.emp_id == a.emp_id\n",
    "         \"\"\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Join With Multiple Columns & Conditions\n",
    "df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\"), (3, \"C\")], [\"A1\", \"A2\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"F\"), (2, \"B\")], [\"B1\", \"B2\"])\n",
    "\n",
    "df = df1.join(df2, (df1.A1 == df2.B1) & (df1.A2 == df2.B2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. The code block is intended to perform an outer join of DataFrames transactionsDf and itemsDf on columns productId and itemId, respectively. Find the error.\n",
    "\n",
    "Code block:\n",
    "\n",
    "transactionsDf.join(itemsDf, [itemsDf.itemId, transactionsDf.productId], \"outer”)\n",
    "\n",
    "- The \"outer\" argument should be eliminated, since \"outer\" is the default join type.\n",
    "- The join type needs to be appended to the join() operator, like join().outer() instead of listing it as the last argument inside the join() call.\n",
    "- The term [itemsDf.itemId, transactionsDf.productId] should be replaced by itemsDf.itemId == transactionsDf.productId.\n",
    "- The term [itemsDf.itemId, transactionsDf.productId] should be replaced by itemsDf.col(\"itemId\") == transactionsDf.col(\"productId\").\n",
    "- The \"outer\" argument should be eliminated from the call and join should be replaced by joinOuter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error [df1.A1, df2.B1]\n",
    "df1.join(df2, [df1.A1, df2.B1], \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct \n",
    "cond = [df1.A1 == df2.B1]\n",
    "df1.join(df2, cond, 'outer').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', ['blue', 'winter', 'cozy'], 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', ['red', 'summer'], 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', ['green', 'summer'], 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"attributes\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 3, 4, 25, 1, None, 1587915332),\n",
    "         (2, 6, 7, 2, 2, None, 1586815312),\n",
    "         (3, 3, None, 25, 3, None, 1585824821),\n",
    "         (4, None, None, 3, 2, None, 1583244275),\n",
    "         (5, None, None, None, 2, None, 1575285427),\n",
    "         (6, 3, 2, 25, 2, None, 1572733275)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True),\n",
    "                     StructField('transactionDate', LongType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.join(transactionsDf, itemsDf.itemId == transactionsDf.transactionId, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AssertionError: how should be basestring\n",
    "itemsDf.join(transactionsDf, \"inner\", itemsDf.itemId == transactionsDf.transactionId)\n",
    "\n",
    "# NameError: name 'itemId' is not defined\n",
    "itemsDf.join(transactionsDf, itemId == transactionId)\n",
    "\n",
    "# AnalysisException: USING column `itemsDf.itemId == transactionsDf.transactionId` \n",
    "# cannot be resolved on the left side of the join. \n",
    "# The left-side columns: [itemId, itemName, attributes, supplier];\n",
    "itemsDf.join(transactionsDf, \"itemsDf.itemId == transactionsDf.transactionId\", \"inner\")\n",
    "\n",
    "\n",
    "# Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.col. Trace:\n",
    "# py4j.Py4JException: Method col([class org.apache.spark.sql.Column]) does not exist\n",
    "itemsDf.join(transactionsDf, col(itemsDf.itemId) == col(transactionsDf.transactionId))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks concatenates rows of DataFrames transactionsDf and transactionsNewDf, omitting any duplicates?\n",
    ">\n",
    "- `transactionsDf.concat(transactionsNewDf).unique()`\n",
    "- `transactionsDf.union(transactionsNewDf).distinct()`\n",
    "- `spark.union(transactionsDf, transactionsNewDf).distinct()`\n",
    "- `transactionsDf.join(transactionsNewDf, how=\"union\").distinct()`\n",
    "- `transactionsDf.union(transactionsNewDf).unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsNewDf = transactionsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.union(transactionsNewDf).distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'DataFrame' object has no attribute 'concat'\n",
    "transactionsDf.concat(transactionsNewDf).unique()\n",
    "\n",
    "# AttributeError: 'SparkSession' object has no attribute 'union'\n",
    "spark.union(transactionsDf, transactionsNewDf).distinct()\n",
    "\n",
    "# IllegalArgumentException: Unsupported join type 'union'.\n",
    "transactionsDf.join(transactionsNewDf, how=\"union\").distinct()\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'unique'\n",
    "transactionsDf.union(transactionsNewDf).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a DataFrame that is an inner join of DataFrame itemsDf and DataFrame transactionsDf, on columns itemId and productId, respectively and in which every itemId just appears once?\n",
    ">\n",
    "- `itemsDf.join(transactionsDf, \"itemsDf.itemId==transactionsDf.productId\").distinct(\"itemId\")`\n",
    "- `itemsDf.join(transactionsDf, itemsDf.itemId==transactionsDf.productId).dropDuplicates([\"itemId\"])`\n",
    "- `itemsDf.join(transactionsDf, itemsDf.itemId==transactionsDf.productId).dropDuplicates(\"itemId\")`\n",
    "- `itemsDf.join(transactionsDf, itemsDf.itemId==transactionsDf.productId, how=\"inner\").distinct([\"itemId\"])`\n",
    "- `itemsDf.join(transactionsDf, \"itemsDf.itemId==transactionsDf.productId\", how=\"inner\").dropDuplicates([\"itemId\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.join(transactionsDf, itemsDf.itemId==transactionsDf.productId).dropDuplicates([\"itemId\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnalysisException: USING column `itemsDf.itemId==transactionsDf.productId` cannot be resolved on the left side of the join. The left-side columns: [itemId, itemName, attributes, supplier]\n",
    "itemsDf.join(transactionsDf, \"itemsDf.itemId==transactionsDf.productId\").distinct(\"itemId\")\n",
    "\n",
    "# Py4JError: An error occurred while calling z:org.apache.spark.api.python.PythonUtils.toSeq. Trace:\n",
    "# py4j.Py4JException: Method toSeq([class java.lang.String]) does not exist\n",
    "itemsDf.join(transactionsDf, itemsDf.itemId==transactionsDf.productId).dropDuplicates(\"itemId\")\n",
    "\n",
    "# TypeError: distinct() takes 1 positional argument but 2 were given\n",
    "itemsDf.join(transactionsDf, itemsDf.itemId==transactionsDf.productId, how=\"inner\").distinct([\"itemId\"])\n",
    "\n",
    "# AnalysisException: USING column `itemsDf.itemId==transactionsDf.productId` cannot be resolved on the left side of the join. The left-side columns: [itemId, itemName, attributes, supplier]\n",
    "itemsDf.join(transactionsDf, \"itemsDf.itemId==transactionsDf.productId\", how=\"inner\").dropDuplicates([\"itemId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which order should the code blocks shown below be run in order to return the number of records that are not empty in column value in the DataFrame resulting from an inner join of DataFrame transactionsDf and itemsDf on columns productId and itemId, respectively?\n",
    ">\n",
    "- `1. .filter(~isnull(col(‘value’)))`\n",
    "- `2. .count()`\n",
    "- `3. transactionsDf.join(itemsDf, col(\"transactionsDf.productId”)==col(“itemsDf.itemId”))`\n",
    "- `4. transactionsDf.join(itemsDf, transactionsDf.productId==itemsDf.itemId, how=’inner’)`\n",
    "- `5. .filter(col(‘value’).isnotnull())`\n",
    "- `6. .sum(col(‘value’))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.join(itemsDf, transactionsDf.productId==itemsDf.itemId, how='inner') \\\n",
    "              .filter(~isnull(col('value'))) \\\n",
    "              .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have the following code block for joining two DataFrames and selecting some columns after join.\n",
    "joinType = “inner”\n",
    ">\n",
    "- `joinExpr = df1.BatchID == df2.BatchID`\n",
    "- `df1.join(df2, joinExpr, joinType).select(“BatchID”, “Year”).show()`\n",
    ">\n",
    "Choose the correct statement about the above code block.\n",
    ">\n",
    "- `The code will apply inner join df1 and df2 and show joined records.`\n",
    "- `There is a syntax error in this code`\n",
    "- `The joinExpr are is incorrect`\n",
    "- `The code block will fail with error : Reference 'BatchID' is ambiguous`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"inner\"\n",
    "joinExpr = empDF.emp_id == addDF.emp_id\n",
    "\n",
    "empDF.join(addDF, joinExpr, joinType).select(\"emp_id\", \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given two DataFrames. The first DataFrame df1 is shown below.\n",
    ">\n",
    "- `+——+———–+——+`\n",
    "- `| Name | Department|Salary|`\n",
    "- `+——+———–+——+`\n",
    "- `| John | Accounts | 5000 |`\n",
    "- `|Sheela|Development| 5500 |`\n",
    "- `+——+———–+——+`\n",
    ">\n",
    "The second DataFrame df2 is shown below.\n",
    ">\n",
    "- `+——+———–+——+`\n",
    "- `| Name | Department|Salary|`\n",
    "- `+——+———–+——+`\n",
    "- `| John | Accounts | 5000 |`\n",
    "- `|Sheela|Development| 5500 |`\n",
    "- `+——+———–+——+`\n",
    ">\n",
    "You want to combine these two data frames and eliminate any duplicates. You know UNION operation in Spark SQL combines two tables and also removes duplicates. How will you do the same in Spark DataFrame API? Choose the correct option.\n",
    ">\n",
    "- `df3 = df1.union(df2)`\n",
    "- `df3 = df1.union(df2).unique()`\n",
    "- `df3 = df1.union(df2).distinct()`\n",
    "- `df3 = df1.union(df2).deleteDuplicates()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emp Table\n",
    "data = [(\"John\",\"Accounts\", 5000), (\"John\",\"Development\", 5500)]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, columns)\n",
    "df2 = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+\n",
      "|Name| Department|Salary|\n",
      "+----+-----------+------+\n",
      "|John|   Accounts|  5000|\n",
      "|John|Development|  5500|\n",
      "|John|   Accounts|  5000|\n",
      "|John|Development|  5500|\n",
      "+----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.union(df2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------+\n",
      "|Name| Department|Salary|\n",
      "+----+-----------+------+\n",
      "|John|Development|  5500|\n",
      "|John|   Accounts|  5000|\n",
      "+----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.union(df2).distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'DataFrame' object has no attribute 'unique'\n",
    "df3 = df1.union(df2).unique()\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'deleteDuplicates'\n",
    "df3 = df1.union(df2).deleteDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('X1','2021','Scala',270),\n",
    "        ('Y5','2021','Scala',230),\n",
    "        ('N3','2020','Scala',150),\n",
    "        ('C5','2020','Scala',100),\n",
    "        ('D7','2020','Python',300),\n",
    "        ('D3','2021','Python',400),\n",
    "        ('H2','2021','Python',500)]\n",
    "\n",
    "columns = [\"BatchID\", \"Year\", \"CourseName\", \"Students\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('X1','Scala',270),\n",
    "        ('Y5','Scala',230),\n",
    "        ('N3','Scala',150),\n",
    "        ('C5','Scala',100),\n",
    "        ('D7','Python',300),\n",
    "        ('D3','Python',400),\n",
    "        ('H2','Python',500)]\n",
    "\n",
    "columns = [\"BatchID\", \"CourseName\", \"Students\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|BatchID|Year|\n",
      "+-------+----+\n",
      "|     D7|2020|\n",
      "|     N3|2020|\n",
      "|     D3|2021|\n",
      "|     X1|2021|\n",
      "|     C5|2020|\n",
      "|     Y5|2021|\n",
      "|     H2|2021|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\" \n",
    "joinExpr = df1.BatchID == df2.BatchID \n",
    "df1.join(df2, joinExpr, joinType).select(df1.BatchID, df1.Year).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|BatchID|Year|\n",
      "+-------+----+\n",
      "|     D7|2020|\n",
      "|     N3|2020|\n",
      "|     D3|2021|\n",
      "|     X1|2021|\n",
      "|     C5|2020|\n",
      "|     Y5|2021|\n",
      "|     H2|2021|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\" \n",
    "joinExpr = \"BatchID\" \n",
    "df1.join(df2, joinExpr, joinType).select(\"BatchID\", \"Year\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|BatchID|Year|\n",
      "+-------+----+\n",
      "|     D7|2020|\n",
      "|     N3|2020|\n",
      "|     D3|2021|\n",
      "|     X1|2021|\n",
      "|     C5|2020|\n",
      "|     Y5|2021|\n",
      "|     H2|2021|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"inner\" \n",
    "joinExpr = df1.BatchID == df2.BatchID \n",
    "df1.join(df2, joinExpr, joinType).select(df1.BatchID, df1.Year).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
