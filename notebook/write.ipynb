{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-write\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "        ('Michael','Rose','','2000-05-19','M',4000),\n",
    "        ('Robert','','Williams','1978-09-05','M',4000),\n",
    "        ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "        ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # saves Dataframe as a CSV file\n",
    "df.write.format(\"csv\").mode(\"ignore\").save(\"../files/spark_output/datacsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame data to CSV file\n",
    "# AnalysisException: path ...datacsv.csv already exists.;\n",
    "df.write.csv(\"../files/datacsv.csv\")\n",
    "\n",
    "# saves Dataframe as a CSV file and throws an error if a file already exists in the location\n",
    "# AnalysisException: path ...datacsv.csv already exists.;\n",
    "df.write.format(\"csv\").mode(\"error\").save(\"../files/datacsv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. The code block is intended to write DataFrame transactionsDf to disk as a parquet file in location /FileStore/transactions_split, using column storeId as key for partitioning. Find the error.\n",
    ">\n",
    "\n",
    "Code block:\n",
    "- `transactionsDf.write.format(“parquet”).partitionOn(“storeId”).save(“/FileStore/transactions_split”)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Partitioning data by storeId is possible with the partitionBy expression, so partitionOn should be replaced by partitionBy.\n",
    "df.write.format(\"parquet\").mode(\"ignore\").partitionBy(\"firstname\").save(\"../files/spark_output/data_split.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error: 'DataFrameWriter' object has no attribute 'partitionOn'\n",
    "df.write.format(\"parquet\").partitionOn(\"firstname\").save(\"../files/spark_output/data_split.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block shown below should write DataFrame transactionsDf as a parquet file to path storeDir, using brotli compression and replacing any previously existing file. Choose the answer that correctly fills the blanks in the code block to accomplish this.\n",
    ">\n",
    "- `transactionsDf.__1__.format(“parquet”).__2__(__3__).option(__4__, “brotli”).__5__(storeDir)`\n",
    ">\n",
    "- `1. save 2. mode 3. \"ignore\" 4. \"compression\" 5. path`\n",
    "- `1. store 2. with 3. \"replacement\" 4. \"compression\" 5. path`\n",
    "- `1. write 2. mode 3. \"overwrite\" 4. \"compression\" 5. save`\n",
    "- `1. save 2. mode 3. \"replace\" 4. \"compression\" 5. path`\n",
    "- `1. write 2. mode 3. \"overwrite\" 4. compression 5. parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 3, 4, 25, 1, None, 1587915332),\n",
    "         (2, 6, 7, 2, 2, None, 1586815312),\n",
    "         (3, 3, None, 25, 3, None, 1585824821),\n",
    "         (4, None, None, 3, 2, None, 1583244275),\n",
    "         (5, None, None, None, 2, None, 1575285427),\n",
    "         (6, 3, 2, 25, 2, None, 1572733275)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True),\n",
    "                     StructField('transactionDate', LongType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.write.format(\"parquet\").mode(\"overwrite\").option(\"compression\", \"brotli\").save(\"../files/spark_output/data_compression.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'DataFrame' object has no attribute 'save'\n",
    "transactionsDf.save.format(\"parquet\").mode(\"ignore\").option(\"compression\", \"brotli\").path(\"../files/spark_output/data_compression.parquet\")\n",
    "\n",
    "# SyntaxError: invalid syntax with\n",
    "transactionsDf.store.format(\"parquet\").with(\"replacement\").option(\"compression\", \"brotli\").path(\"../files/spark_output/data_compression.parquet\")\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'save'\n",
    "transactionsDf.save.format(\"parquet\").mode(\"replace\").option(\"compression\", \"brotli\").path(\"../files/spark_output/data_compression.parquet\")\n",
    "\n",
    "# NameError: name 'compression' is not defined\n",
    "transactionsDf.write.format(\"parquet\").mode(\"overwrite\").option(compression, \"brotli\").parquet(\"../files/spark_output/data_compression.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks silently writes DataFrame itemsDf in avro format to location fileLocation if a file does not yet exist at that location?\n",
    ">\n",
    "- `itemsDf.write.avro(fileLocation)`\n",
    "- `itemsDf.write.format(\"avro\").mode(\"ignore\").save(fileLocation)`\n",
    "- `itemsDf.write.format(\"avro\").mode(\"errorifexists\").save(fileLocation)`\n",
    "- `itemsDf.save.format(\"avro\").mode(\"ignore\").write(fileLocation)`\n",
    "- `spark.DataFrameWriter(itemsDf).format(\"avro\").write(fileLocation)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.write.format(\"avro\").mode(\"ignore\").save(\"../files/spark_output/data_avro.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'DataFrameWriter' object has no attribute 'avro'\n",
    "itemsDf.write.avro(\"../files/spark_output/data_avro.avro\")\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'save'\n",
    "itemsDf.save.format(\"avro\").mode(\"ignore\").write(\"../files/spark_output/data_avro.avro\")\n",
    "\n",
    "# AttributeError: 'SparkSession' object has no attribute 'DataFrameWriter'\n",
    "spark.DataFrameWriter(itemsDf).format(\"avro\").write(\"../files/spark_output/data_avro.avro\")\n",
    "\n",
    "# Error: errorifexists\n",
    "itemsDf.write.format(\"avro\").mode(\"errorifexists\").save(\"../files/spark_output/data_avro.avro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.ExcelWriter('../files/excel_output.xlsx', engine='xlsxwriter') as writer:\n",
    "    transactionsDf.toPandas().to_excel(writer, sheet_name='Sheet_Transaction')\n",
    "    itemsDf.toPandas().to_excel(writer, sheet_name='Sheet_Items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
