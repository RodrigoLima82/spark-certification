{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER() + WHERE() + ISIN() + CONTAIN() + WHEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-filter-isin\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dept = [(\"Finance\",10), (\"Marketing\",20), (\"Sales\",30), (\"IT\",40), (\"X\",40), (\"Y\",40), (\"Z\",40)]\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The argument to the where method cannot be a string.\n",
    "deptDF.where(\"col(dept_id) >= 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.where(col('dept_id') >= 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. The code block should return a DataFrame where all entries in column supplier contain the letter combination et in this order. Find the error.\n",
    ">\n",
    "Code block:\n",
    ">\n",
    "- `itemsDf.filter(column(‘supplier’).isin(‘et’))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf[itemsDf.supplier.isin(\"Sports Company Inc.\")].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.filter(col(\"supplier\").contains(\"Company\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a new DataFrame with only columns predError and values of every second row of DataFrame transactionsDf?\n",
    ">\n",
    "Entire DataFrame transactionsDf:\n",
    ">\n",
    "- `+————-+———+—–+——-+———+—-+`\n",
    "- `|transactionId|predError|value|storeId|productId| f|`\n",
    "- `+————-+———+—–+——-+———+—-+`\n",
    "- `| 1| 3| 4| 25| 1|null|`\n",
    "- `| 2| 6| 7| 2| 2|null|`\n",
    "- `| 3| 3| null| 25| 3|null|`\n",
    "- `| 4| null| null| 3| 2|null|`\n",
    "- `| 5| null| null| null| 2|null|`\n",
    "- `| 6| 3| 2| 25| 2|null|`\n",
    "- `+————-+———+—–+——-+———+—-+`\n",
    ">\n",
    "- `transactionsDf.filter(col(\"transactionId\").isin([3,4,6])).select([predError, value])`\n",
    "- `transactionsDf.select(col(\"transactionId\").isin([3,4,6]), \"predError\", \"value\")`\n",
    "- `transactionsDf.filter(\"transactionId\" % 2 == 0).select(\"predError\", \"value\")`\n",
    "- `transactionsDf.filter(col(\"transactionId\") % 2 == 0).select(\"predError\", \"value\")`\n",
    "- `transactionsDf.createOrReplaceTempView(\"transactionsDf\")`\n",
    "- `spark.sql(\"FROM transactionsDf SELECT predError, value WHERE transactionId % 2 = 0\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------+---------+----+\n",
      "|transactionId|predError|value|storeId|productId|   f|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "|            1|        3|    4|     25|        1|null|\n",
      "|            2|        6|    7|      2|        2|null|\n",
      "|            3|        3| null|     25|        3|null|\n",
      "|            4|     null| null|      3|        2|null|\n",
      "|            5|     null| null|   null|        2|null|\n",
      "|            6|        3|    2|     25|        2|null|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 3, 4, 25, 1, None),\n",
    "        (2, 6, 7, 2, 2, None),\n",
    "        (3, 3, None, 25, 3, None),\n",
    "        (4, None, None, 3, 2, None),\n",
    "        (5, None, None, None, 2, None),\n",
    "        (6, 3, 2, 25, 2, None)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)\n",
    "transactionsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.filter((col(\"productId\")==3) | (col(\"productId\")<1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-fd3de26aeb47>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-fd3de26aeb47>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    transactionsDf.where(\"productId\"=3).or(\"productId\"<1))\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# NameError: name 'productId' is not defined\n",
    "transactionsDf.filter(productId==3 or productId<1)\n",
    "\n",
    "# ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not'\n",
    "transactionsDf.filter((col(\"productId\")==3) or (col(\"productId\")<1))\n",
    "\n",
    "# Py4JError: An error occurred while calling o43.or. \n",
    "# Trace: py4j.Py4JException: Method or([class java.lang.Integer]) does not exist\n",
    "transactionsDf.filter(col(\"productId\")==3 | col(\"productId\")<1)\n",
    "\n",
    "# SyntaxError: invalid syntax \"or\"\n",
    "transactionsDf.where(\"productId\"=3).or(\"productId\"<1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER() + WHERE() + ISIN() + CONTAIN() + WHEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-filter-isin\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dept = [(\"Finance\",10), (\"Marketing\",20), (\"Sales\",30), (\"IT\",40), (\"X\",40), (\"Y\",40), (\"Z\",40)]\n",
    "deptColumns = [\"dept_name\", \"dept_id\"]\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The argument to the where method cannot be a string.\n",
    "deptDF.where(\"col(dept_id) >= 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deptDF.where(col('dept_id') >= 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. The code block should return a DataFrame where all entries in column supplier contain the letter combination et in this order. Find the error.\n",
    ">\n",
    "Code block:\n",
    ">\n",
    "- `itemsDf.filter(column(‘supplier’).isin(‘et’))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf[itemsDf.supplier.isin(\"Sports Company Inc.\")].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.filter(col(\"supplier\").contains(\"Company\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a new DataFrame with only columns predError and values of every second row of DataFrame transactionsDf?\n",
    ">\n",
    "Entire DataFrame transactionsDf:\n",
    ">\n",
    "- `+————-+———+—–+——-+———+—-+`\n",
    "- `|transactionId|predError|value|storeId|productId| f|`\n",
    "- `+————-+———+—–+——-+———+—-+`\n",
    "- `| 1| 3| 4| 25| 1|null|`\n",
    "- `| 2| 6| 7| 2| 2|null|`\n",
    "- `| 3| 3| null| 25| 3|null|`\n",
    "- `| 4| null| null| 3| 2|null|`\n",
    "- `| 5| null| null| null| 2|null|`\n",
    "- `| 6| 3| 2| 25| 2|null|`\n",
    "- `+————-+———+—–+——-+———+—-+`\n",
    ">\n",
    "- `transactionsDf.filter(col(\"transactionId\").isin([3,4,6])).select([predError, value])`\n",
    "- `transactionsDf.select(col(\"transactionId\").isin([3,4,6]), \"predError\", \"value\")`\n",
    "- `transactionsDf.filter(\"transactionId\" % 2 == 0).select(\"predError\", \"value\")`\n",
    "- `transactionsDf.filter(col(\"transactionId\") % 2 == 0).select(\"predError\", \"value\")`\n",
    "- `transactionsDf.createOrReplaceTempView(\"transactionsDf\")`\n",
    "- `spark.sql(\"FROM transactionsDf SELECT predError, value WHERE transactionId % 2 = 0\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------+---------+----+\n",
      "|transactionId|predError|value|storeId|productId|   f|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "|            1|        3|    4|     25|        1|null|\n",
      "|            2|        6|    7|      2|        2|null|\n",
      "|            3|        3| null|     25|        3|null|\n",
      "|            4|     null| null|      3|        2|null|\n",
      "|            5|     null| null|   null|        2|null|\n",
      "|            6|        3|    2|     25|        2|null|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 3, 4, 25, 1, None),\n",
    "        (2, 6, 7, 2, 2, None),\n",
    "        (3, 3, None, 25, 3, None),\n",
    "        (4, None, None, 3, 2, None),\n",
    "        (5, None, None, None, 2, None),\n",
    "        (6, 3, 2, 25, 2, None)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)\n",
    "transactionsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'productId' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-3-75f5ffb4f611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactionsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproductId\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mproductId\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'productId' is not defined"
     ]
    }
   ],
   "source": [
    "# NameError: name 'productId' is not defined\n",
    "transactionsDf.filter(productId==3 or productId<1)\n",
    "\n",
    "# ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not'\n",
    "transactionsDf.filter((col(\"productId\")==3) or (col(\"productId\")<1))\n",
    "\n",
    "# Py4JError: An error occurred while calling o43.or. \n",
    "# Trace: py4j.Py4JException: Method or([class java.lang.Integer]) does not exist\n",
    "transactionsDf.filter(col(\"productId\")==3 | col(\"productId\")<1)\n",
    "\n",
    "# SyntaxError: invalid syntax \"or\"\n",
    "transactionsDf.where(\"productId\"=3).or(\"productId\"<1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o43.or. Trace:\npy4j.Py4JException: Method or([class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n\tat py4j.Gateway.invoke(Gateway.java:286)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-5-98363d3db53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactionsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"productId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"productId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(self, other)\u001b[0m\n",
      "\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    117\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mnjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    120\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n",
      "\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n",
      "\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n",
      "\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 330\u001b[0;31m                 raise Py4JError(\n",
      "\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
      "\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o43.or. Trace:\n",
      "py4j.Py4JException: Method or([class java.lang.Integer]) does not exist\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n",
      "\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:286)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-1e9072805d47>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-1e9072805d47>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n",
      "\u001b[0;31m    transactionsDf.where(\"productId\"=3).or(\"productId\"<1))\u001b[0m\n",
      "\u001b[0m                                        ^\u001b[0m\n",
      "\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------+---------+----+\n",
      "|transactionId|predError|value|storeId|productId|   f|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "|            3|        3| null|     25|        3|null|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.select(col(\"transactionId\").isin([3,4,6]), \"predError\", \"value\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.filter(col(\"transactionId\") % 2 == 0).select(\"predError\", \"value\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"FROM transactionsDf SELECT predError, value WHERE transactionId % 2 = 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'predError' is not defined\n",
    "# transactionsDf.filter(col(\"transactionId\").isin([3,4,6])).select([predError, value])\n",
    "\n",
    "# TypeError: not all arguments converted during string formatting\n",
    "# transactionsDf.filter(\"transactionId\" % 2 == 0).select(\"predError\", \"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = col(\"productId\").isin(1)\n",
    "cond2 = col(\"productId\").isin(2)\n",
    "cond3 = col(\"productId\").isin(3)\n",
    "\n",
    "transactionsDf.withColumn(\"productGroup\", when(cond1, \"Grupo1\")\n",
    "\t\t\t\t\t\t\t\t         .when(cond2, \"Grupo2\")\n",
    "\t\t\t\t\t\t\t\t         .when(cond3, \"Grupo3\")\n",
    "\t\t\t\t\t\t\t\t         .otherwise(\"Grupo4\")) \\\n",
    "\t\t\t\t\t\t\t\t  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 13), \n",
    "        (2, 23),\n",
    "\t\t(3, 10),\n",
    "\t\t(4, 17),\n",
    "\t\t(5, 18),\n",
    "\t\t(6, 21),\n",
    "\t\t(7, 9),\n",
    "\t\t(8, 26),\n",
    "\t\t(9, 28),\n",
    "\t\t\n",
    "    ], [\"id\", \"Idade\"])\n",
    "\n",
    "cond1 = col(\"Idade\").isin(10, 11, 12)\n",
    "cond2 = col(\"Idade\").isin(13, 14, 15)\n",
    "cond3 = col(\"Idade\").isin(16, 17, 18)\n",
    "\n",
    "df.withColumn(\"Idade_Grupo\", when(cond1, \"Grupo1\")\n",
    "                            .when(cond2, \"Grupo2\")\n",
    "                            .when(cond3, \"Grupo3\")\n",
    "                            .otherwise(\"Grupo4\")) \\\n",
    "\t\t\t .show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a one-column DataFrame of all values in column supplier of DataFrame itemsDf that do not contain the letter X? In the DataFrame, every value should only be listed once.\n",
    ">\n",
    "Sample of DataFrame itemsDf:\n",
    ">\n",
    "- `+——+——————–+——————–+——————-+`\n",
    "- `|itemId| itemName| attributes| supplier|`\n",
    "- `+——+——————–+——————–+——————-+`\n",
    "- `| 1|Thick Coat for Wa…|[blue, winter, cozy]|Sports Company Inc.|`\n",
    "- `| 2|Elegant Outdoors …|[red, summer, fre…| YetiX|`\n",
    "- `| 3| Outdoors Backpack|[green, summer, t…|Sports Company Inc.|`\n",
    "- `+——+——————–+——————–+——————-+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', ['blue', 'winter', 'cozy'], 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', ['red', 'summer'], 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', ['green', 'summer'], 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"attributes\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.select(~col('supplier').contains('X')).distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.filter(~col('supplier').contains('X')).select('supplier').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'supplier' is not defined\n",
    "itemsDf.filter(col(supplier).not_contains('X')).select(supplier).distinct().show()\n",
    "\n",
    "# ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.\n",
    "itemsDf.filter(not(col('supplier').contains('X'))).select('supplier').unique().show()\n",
    "\n",
    "# SyntaxError: invalid syntax !\n",
    "itemsDf.filter(!col('supplier').contains('X')).select(col('supplier')).unique().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+-------+---------+----+\n",
      "|transactionId|predError|value|storeId|productId|   f|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "|            3|        3| null|     25|        3|null|\n",
      "+-------------+---------+-----+-------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.select(col(\"transactionId\").isin([3,4,6]), \"predError\", \"value\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.filter(col(\"transactionId\") % 2 == 0).select(\"predError\", \"value\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"FROM transactionsDf SELECT predError, value WHERE transactionId % 2 = 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'predError' is not defined\n",
    "# transactionsDf.filter(col(\"transactionId\").isin([3,4,6])).select([predError, value])\n",
    "\n",
    "# TypeError: not all arguments converted during string formatting\n",
    "# transactionsDf.filter(\"transactionId\" % 2 == 0).select(\"predError\", \"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1 = col(\"productId\").isin(1)\n",
    "cond2 = col(\"productId\").isin(2)\n",
    "cond3 = col(\"productId\").isin(3)\n",
    "\n",
    "transactionsDf.withColumn(\"productGroup\", when(cond1, \"Grupo1\")\n",
    "\t\t\t\t\t\t\t\t         .when(cond2, \"Grupo2\")\n",
    "\t\t\t\t\t\t\t\t         .when(cond3, \"Grupo3\")\n",
    "\t\t\t\t\t\t\t\t         .otherwise(\"Grupo4\")) \\\n",
    "\t\t\t\t\t\t\t\t  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 13), \n",
    "        (2, 23),\n",
    "\t\t(3, 10),\n",
    "\t\t(4, 17),\n",
    "\t\t(5, 18),\n",
    "\t\t(6, 21),\n",
    "\t\t(7, 9),\n",
    "\t\t(8, 26),\n",
    "\t\t(9, 28),\n",
    "\t\t\n",
    "    ], [\"id\", \"Idade\"])\n",
    "\n",
    "cond1 = col(\"Idade\").isin(10, 11, 12)\n",
    "cond2 = col(\"Idade\").isin(13, 14, 15)\n",
    "cond3 = col(\"Idade\").isin(16, 17, 18)\n",
    "\n",
    "df.withColumn(\"Idade_Grupo\", when(cond1, \"Grupo1\")\n",
    "                            .when(cond2, \"Grupo2\")\n",
    "                            .when(cond3, \"Grupo3\")\n",
    "                            .otherwise(\"Grupo4\")) \\\n",
    "\t\t\t .show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a one-column DataFrame of all values in column supplier of DataFrame itemsDf that do not contain the letter X? In the DataFrame, every value should only be listed once.\n",
    ">\n",
    "Sample of DataFrame itemsDf:\n",
    ">\n",
    "- `+——+——————–+——————–+——————-+`\n",
    "- `|itemId| itemName| attributes| supplier|`\n",
    "- `+——+——————–+——————–+——————-+`\n",
    "- `| 1|Thick Coat for Wa…|[blue, winter, cozy]|Sports Company Inc.|`\n",
    "- `| 2|Elegant Outdoors …|[red, summer, fre…| YetiX|`\n",
    "- `| 3| Outdoors Backpack|[green, summer, t…|Sports Company Inc.|`\n",
    "- `+——+——————–+——————–+——————-+`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', ['blue', 'winter', 'cozy'], 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', ['red', 'summer'], 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', ['green', 'summer'], 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"attributes\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.select(~col('supplier').contains('X')).distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsDf.filter(~col('supplier').contains('X')).select('supplier').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'supplier' is not defined\n",
    "itemsDf.filter(col(supplier).not_contains('X')).select(supplier).distinct().show()\n",
    "\n",
    "# ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.\n",
    "itemsDf.filter(not(col('supplier').contains('X'))).select('supplier').unique().show()\n",
    "\n",
    "# SyntaxError: invalid syntax !\n",
    "itemsDf.filter(!col('supplier').contains('X')).select(col('supplier')).unique().show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
