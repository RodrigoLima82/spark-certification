{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COALESCE() + REPARTITION()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example creates 5 partitions as specified in `master(\"local[5]\")` and the data is distributed across all these 5 partitions.\n",
    "\n",
    "- `Partition 1: 0 1 2 3`\n",
    "- `Partition 2: 4 5 6 7`\n",
    "- `Partition 3: 8 9 10 11`\n",
    "- `Partition 4: 12 13 14 15`\n",
    "- `Partition 5: 16 17 18 19`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-coalesce-repartition\")\n",
    "    #.master(\"local[5]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").csv(\"../files/partition/partition.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame repartition()\n",
    "- repartition() method is used to increase or decrease the partitions. \n",
    "- the below example increases the partitions from 5 to 6 by moving data from all partitions.\n",
    "\n",
    ">\n",
    "- `Partition 1 : 14 1 5`\n",
    "- `Partition 2 : 4 16 15`\n",
    "- `Partition 3 : 8 3 18`\n",
    "- `Partition 4 : 12 2 19`\n",
    "- `Partition 5 : 6 17 7 0`\n",
    "- `Partition 6 : 9 10 11 13`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame coalesce()\n",
    "- coalesce() is used only to `decrease` the number of partitions. \n",
    "- this is an optimized or improved version of repartition() where the movement of the data across the partitions is fewer using coalesce.\n",
    "- the below example we are reducing 5 to 2 partitions, the data movement happens only from 3 partitions and it moves to remain 2 partitions.\n",
    "\n",
    ">\n",
    "- `Partition 1 : 0 1 2 3 8 9 10 11`\n",
    "- `Partition 2 : 4 5 6 7 12 13 14 15 16 17 18 19`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Shuffle Partition\n",
    "\n",
    "- Calling `groupBy()`, `union()`, `join()` and similar functions on DataFrame results in shuffling data between multiple executors and even machines and finally repartitions data into `200 partitions by default`. \n",
    "- PySpark default defines shuffling partition to 200 using `spark.sql.shuffle.partitions` configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "df4 = df.groupBy(\"id\").count()\n",
    "print(df4.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks `reduces` a DataFrame from 12 to 6 partitions and performs a full shuffle?\n",
    ">\n",
    "- `DataFrame.repartition(12)`\n",
    "- `DataFrame.coalesce(6).shuffle()`\n",
    "- `DataFrame.coalesce(6)`\n",
    "- `DataFrame.coalesce(6, shuffle=True)`\n",
    "- `DataFrame.repartition(6)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. When the code block below has executed, it should have divided DataFrame transactionsDf into 14 parts, based on columns storeId and transactionDate (in this order). Find the error.\n",
    "\n",
    ">\n",
    "Code block:\n",
    "- `transactionsDf.coalesce(14, (“storeId”, “transactionDate”))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 3, 4, 25, 1, None, 1587915332),\n",
    "         (2, 6, 7, 2, 2, None, 1586815312),\n",
    "         (3, 3, None, 25, 3, None, 1585824821),\n",
    "         (4, None, None, 3, 2, None, 1583244275),\n",
    "         (5, None, None, None, 2, None, 1575285427),\n",
    "         (6, 3, 2, 25, 2, None, 1572733275)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True),\n",
    "                     StructField('transactionDate', LongType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "coalesce() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b4f3c91000de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TypeError: coalesce() takes 2 positional arguments but 3 were given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtransactionsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'storeId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transactionDate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: coalesce() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# TypeError: coalesce() takes 2 positional arguments but 3 were given\n",
    "transactionsDf.coalesce(14, ('storeId', 'transactionDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transactionId: int, predError: int, value: int, storeId: int, productId: int, f: int, transactionDate: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Operator coalesce needs to be replaced by repartition and the parentheses around the column names need to be replaced by square brackets.\n",
    "transactionsDf.repartition(14, ['storeId', 'transactionDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block shown below should write DataFrame transactionsDf to disk at path csvPath as a single CSV file, using tabs ( characters) as separators between columns, expressing missing values as string n/a, and omitting a header row with column names. \n",
    ">\n",
    "Choose the answer that correctly fills the blanks in the code block to accomplish this.\n",
    ">\n",
    "\n",
    "- `transactionsDf.__1__.write.__2__(__3__, ” “).__4__.__5__(csvPath)`\n",
    ">\n",
    "- `1. coalesce(1) 2. option 3. \"sep\" 4. option(\"header\", True) 5. path`\n",
    "- `1. coalesce(1) 2. option 3. \"colsep\" 4. option(\"nullValue\", \"n/a\") 5. path`\n",
    "- `1. repartition(1) 2. option 3. \"sep\" 4. option(\"nullValue\", \"n/a\") 5. csv`\n",
    "- `1. csv 2. option 3. \"sep\" 4. option(\"emptyValue\", \"n/a\") 5. path`\n",
    "- `1. repartition(1) 2. mode 3. \"sep\" 4. mode(\"nullValue\", \"n/a\") 5. csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a865757d1c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactionsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../files/partition/partition2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "# AttributeError: 'DataFrameWriter' object has no attribute 'path'\n",
    "transactionsDf.coalesce(1).write.option(\"sep\", \" \").option(\"header\", True).path(\"../files/partition/partition2.csv\")\n",
    "transactionsDf.coalesce(1).write.option(\"colsep\", \" \").option(\"nullValue\", \"n/a\").path(\"../files/partition/partition2.csv\")\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'csv'\n",
    "transactionsDf.csv.write.option(\"sep\", \" \").option(\"emptyValue\", \"n/a\").path(\"../files/partition/partition2.csv\")\n",
    "\n",
    "# TypeError: mode() takes 2 positional arguments but 3 were give\n",
    "transactionsDf.repartition(1).write.mode(\"sep\", \" \").mode(\"nullValue\", \"n/a\").csv(\"../files/partition/partition2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.repartition(1).write.option(\"sep\", \" \").option(\"nullValue\", \"n/a\").csv(\"../files/partition/partition2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
