{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-join\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|emp_id|    name|emp_dept_id|\n",
      "+------+--------+-----------+\n",
      "|     1|   Smith|         10|\n",
      "|     2|    Rose|         20|\n",
      "|     3|Williams|         10|\n",
      "|     4|   Jones|         30|\n",
      "+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Emp Table\n",
    "empData = [(1,\"Smith\",10), (2,\"Rose\",20),(3,\"Williams\",10), (4,\"Jones\",30)]\n",
    "\n",
    "empColumns = [\"emp_id\",\"name\",\"emp_dept_id\"]\n",
    "\n",
    "empDF = spark.createDataFrame(empData,empColumns)\n",
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|  Finance|     10|\n",
      "|Marketing|     20|\n",
      "|    Sales|     30|\n",
      "|       IT|     40|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dept Table\n",
    "deptData = [(\"Finance\",10), (\"Marketing\",20), (\"Sales\",30),(\"IT\",40)]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "\n",
    "deptDF=spark.createDataFrame(deptData,deptColumns)  \n",
    "deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------+-----+\n",
      "|emp_id|       addline1|    city|state|\n",
      "+------+---------------+--------+-----+\n",
      "|     1|   1523 Main St|     SFO|   CA|\n",
      "|     2| 3453 Orange St|     SFO|   NY|\n",
      "|     3|   34 Warner St|  Jersey|   NJ|\n",
      "|     4|221 Cavalier St|  Newark|   DE|\n",
      "|     5|  789 Walnut St|Sandiago|   CA|\n",
      "+------+---------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Address Table\n",
    "addData=[(1,\"1523 Main St\",\"SFO\",\"CA\"),\n",
    "         (2,\"3453 Orange St\",\"SFO\",\"NY\"),\n",
    "         (3,\"34 Warner St\",\"Jersey\",\"NJ\"),\n",
    "         (4,\"221 Cavalier St\",\"Newark\",\"DE\"),\n",
    "         (5,\"789 Walnut St\",\"Sandiago\",\"CA\")]\n",
    "\n",
    "addColumns = [\"emp_id\", \"addline1\", \"city\", \"state\"]\n",
    "\n",
    "addDF = spark.createDataFrame(addData,addColumns)\n",
    "addDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Join Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     3|Williams|         10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     2|    Rose|         20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     4|   Jones|         30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join(right, joinExprs, joinType)\n",
    "# join(right)\n",
    "\n",
    "empDF.join(addDF, empDF[\"emp_id\"] == addDF[\"emp_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------------+------+-----+\n",
      "|     1|   Smith|         10|   1523 Main St|   SFO|   CA|\n",
      "|     3|Williams|         10|   34 Warner St|Jersey|   NJ|\n",
      "|     2|    Rose|         20| 3453 Orange St|   SFO|   NY|\n",
      "|     4|   Jones|         30|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop Duplicate Columns After Join\n",
    "empDF.join(addDF,[\"emp_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------------+------+-----+---------+-------+\n",
      "|emp_id|    name|emp_dept_id|       addline1|  city|state|dept_name|dept_id|\n",
      "+------+--------+-----------+---------------+------+-----+---------+-------+\n",
      "|     1|   Smith|         10|   1523 Main St|   SFO|   CA|  Finance|     10|\n",
      "|     3|Williams|         10|   34 Warner St|Jersey|   NJ|  Finance|     10|\n",
      "|     4|   Jones|         30|221 Cavalier St|Newark|   DE|    Sales|     30|\n",
      "|     2|    Rose|         20| 3453 Orange St|   SFO|   NY|Marketing|     20|\n",
      "+------+--------+-----------+---------------+------+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join Multiple DataFrames by chaining\n",
    "empDF.join(addDF,[\"emp_id\"]) \\\n",
    "     .join(deptDF,empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Where for Join Condition\n",
    "empDF.join(deptDF).where(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "    .join(addDF).where(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Filter for Join Condition\n",
    "empDF.join(deptDF).filter(empDF[\"emp_dept_id\"] == deptDF[\"dept_id\"]) \\\n",
    "    .join(addDF).filter(empDF[\"emp_id\"] == addDF[\"emp_id\"]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|emp_id|    name|emp_dept_id|dept_name|dept_id|emp_id|       addline1|  city|state|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "|     1|   Smith|         10|  Finance|     10|     1|   1523 Main St|   SFO|   CA|\n",
      "|     3|Williams|         10|  Finance|     10|     3|   34 Warner St|Jersey|   NJ|\n",
      "|     2|    Rose|         20|Marketing|     20|     2| 3453 Orange St|   SFO|   NY|\n",
      "|     4|   Jones|         30|    Sales|     30|     4|221 Cavalier St|Newark|   DE|\n",
      "+------+--------+-----------+---------+-------+------+---------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL\n",
    "empDF.createOrReplaceTempView(\"emp\")\n",
    "deptDF.createOrReplaceTempView(\"dept\")\n",
    "addDF.createOrReplaceTempView(\"add\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "             select * \n",
    "               from emp e, dept d, add a\n",
    "              where e.emp_dept_id == d.dept_id \n",
    "                and e.emp_id == a.emp_id\n",
    "         \"\"\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| A1| A2| B1| B2|\n",
      "+---+---+---+---+\n",
      "|  2|  B|  2|  B|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark Join With Multiple Columns & Conditions\n",
    "df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\"), (3, \"C\")], [\"A1\", \"A2\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"F\"), (2, \"B\")], [\"B1\", \"B2\"])\n",
    "\n",
    "df = df1.join(df2, (df1.A1 == df2.B1) & (df1.A2 == df2.B2))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. The code block is intended to perform an outer join of DataFrames transactionsDf and itemsDf on columns productId and itemId, respectively. Find the error.\n",
    "\n",
    "Code block:\n",
    "\n",
    "transactionsDf.join(itemsDf, [itemsDf.itemId, transactionsDf.productId], “outer”)\n",
    "\n",
    "- The \"outer\" argument should be eliminated, since \"outer\" is the default join type.\n",
    "- The join type needs to be appended to the join() operator, like join().outer() instead of listing it as the last argument inside the join() call.\n",
    "- The term [itemsDf.itemId, transactionsDf.productId] should be replaced by itemsDf.itemId == transactionsDf.productId.\n",
    "- The term [itemsDf.itemId, transactionsDf.productId] should be replaced by itemsDf.col(\"itemId\") == transactionsDf.col(\"productId\").\n",
    "- The \"outer\" argument should be eliminated from the call and join should be replaced by joinOuter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(`A1` AND `B1`)' due to data type mismatch: '(`A1` AND `B1`)' requires boolean type, not bigint;;\n'Join FullOuter, (A1#831L AND B1#835L)\n:- LogicalRDD [A1#831L, A2#832], false\n+- LogicalRDD [B1#835L, B2#836], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bd817f5f8065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Error [df1.A1, df2.B1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(`A1` AND `B1`)' due to data type mismatch: '(`A1` AND `B1`)' requires boolean type, not bigint;;\n'Join FullOuter, (A1#831L AND B1#835L)\n:- LogicalRDD [A1#831L, A2#832], false\n+- LogicalRDD [B1#835L, B2#836], false\n"
     ]
    }
   ],
   "source": [
    "# Error [df1.A1, df2.B1]\n",
    "df1.join(df2, [df1.A1, df2.B1], \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+----+\n",
      "| A1| A2|  B1|  B2|\n",
      "+---+---+----+----+\n",
      "|  1|  A|   1|   F|\n",
      "|  3|  C|null|null|\n",
      "|  2|  B|   2|   B|\n",
      "+---+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correct \n",
    "cond = [df1.A1 == df2.B1]\n",
    "df1.join(df2, cond, 'outer').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemId: long (nullable = true)\n",
      " |-- itemName: string (nullable = true)\n",
      " |-- attributes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- supplier: string (nullable = true)\n",
      "\n",
      "+------+--------------------+--------------------+-------------------+\n",
      "|itemId|            itemName|          attributes|           supplier|\n",
      "+------+--------------------+--------------------+-------------------+\n",
      "|     1|Thick Coat for Wa...|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|     2|Elegant Outdoors ...|       [red, summer]|              YetiX|\n",
      "|     3|   Outdoors Backpack|     [green, summer]|Sports Company Inc.|\n",
      "+------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', ['blue', 'winter', 'cozy'], 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', ['red', 'summer'], 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', ['green', 'summer'], 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"attributes\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 3, 4, 25, 1, None, 1587915332),\n",
    "         (2, 6, 7, 2, 2, None, 1586815312),\n",
    "         (3, 3, None, 25, 3, None, 1585824821),\n",
    "         (4, None, None, 3, 2, None, 1583244275),\n",
    "         (5, None, None, None, 2, None, 1575285427),\n",
    "         (6, 3, 2, 25, 2, None, 1572733275)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True),\n",
    "                     StructField('transactionDate', LongType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------------+-------------+---------+-----+-------+---------+----+---------------+\n",
      "|itemId|            itemName|          attributes|           supplier|transactionId|predError|value|storeId|productId|   f|transactionDate|\n",
      "+------+--------------------+--------------------+-------------------+-------------+---------+-----+-------+---------+----+---------------+\n",
      "|     1|Thick Coat for Wa...|[blue, winter, cozy]|Sports Company Inc.|            1|        3|    4|     25|        1|null|     1587915332|\n",
      "|     3|   Outdoors Backpack|     [green, summer]|Sports Company Inc.|            3|        3| null|     25|        3|null|     1585824821|\n",
      "|     2|Elegant Outdoors ...|       [red, summer]|              YetiX|            2|        6|    7|      2|        2|null|     1586815312|\n",
      "+------+--------------------+--------------------+-------------------+-------------+---------+-----+-------+---------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.join(transactionsDf, itemsDf.itemId == transactionsDf.transactionId, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "how should be basestring",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7ed2ae511837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mitemsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactionsDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemId\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtransactionsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransactionId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: how should be basestring"
     ]
    }
   ],
   "source": [
    "# AssertionError: how should be basestring\n",
    "itemsDf.join(transactionsDf, \"inner\", itemsDf.itemId == transactionsDf.transactionId)\n",
    "\n",
    "# NameError: name 'itemId' is not defined\n",
    "itemsDf.join(transactionsDf, itemId == transactionId)\n",
    "\n",
    "# AnalysisException: USING column `itemsDf.itemId == transactionsDf.transactionId` \n",
    "# cannot be resolved on the left side of the join. \n",
    "# The left-side columns: [itemId, itemName, attributes, supplier];\n",
    "itemsDf.join(transactionsDf, \"itemsDf.itemId == transactionsDf.transactionId\", \"inner\")\n",
    "\n",
    "\n",
    "# Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.col. Trace:\n",
    "# py4j.Py4JException: Method col([class org.apache.spark.sql.Column]) does not exist\n",
    "itemsDf.join(transactionsDf, col(itemsDf.itemId) == col(transactionsDf.transactionId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------+--------------------+-------------------+-------------+---------+-----+-------+---------+----+---------------+\n",
      "|itemId|itemName                          |attributes          |supplier           |transactionId|predError|value|storeId|productId|f   |transactionDate|\n",
      "+------+----------------------------------+--------------------+-------------------+-------------+---------+-----+-------+---------+----+---------------+\n",
      "|1     |Thick Coat for Walking in the Snow|[blue, winter, cozy]|Sports Company Inc.|1            |3        |4    |25     |1        |null|1587915332     |\n",
      "|3     |Outdoors Backpack                 |[green, summer]     |Sports Company Inc.|3            |3        |null |25     |3        |null|1585824821     |\n",
      "|2     |Elegant Outdoors Summer Dress     |[red, summer]       |YetiX              |2            |6        |7    |2      |2        |null|1586815312     |\n",
      "+------+----------------------------------+--------------------+-------------------+-------------+---------+-----+-------+---------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
