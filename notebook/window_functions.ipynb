{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Window Functions\n",
    "Spark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row. Spark SQL supports three kinds of window functions:\n",
    "\n",
    "- ranking functions\n",
    "- analytic functions\n",
    "- aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-window-functios\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "              (\"Michael\", \"Sales\", 4600),  \\\n",
    "              (\"Robert\", \"Sales\", 4100),   \\\n",
    "              (\"Maria\", \"Finance\", 3000),  \\\n",
    "              (\"James\", \"Sales\", 3000),    \\\n",
    "              (\"Scott\", \"Finance\", 3300),  \\\n",
    "              (\"Jen\", \"Finance\", 3900),    \\\n",
    "              (\"Jeff\", \"Marketing\", 3000), \\\n",
    "              (\"Kumar\", \"Marketing\", 2000),\\\n",
    "              (\"Saif\", \"Sales\", 4100) \\\n",
    "             )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Window Ranking functions\n",
    "### row_number Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dense_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ntile Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Window Analytic functions\n",
    "### cume_dist Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" cume_dist \"\"\"\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
    "   .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lag Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"lag\"\"\"\n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
    "      .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lead Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \"\"\"lead\"\"\"\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Window Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+------+------------------+\n",
      "|Country|CustomerID|PurchaseDate|Amount|CumulativePurchase|\n",
      "+-------+----------+------------+------+------------------+\n",
      "|     US|    536366|  2021-05-14|   200|             200.0|\n",
      "|     US|    536365|  2021-05-15|   600|             800.0|\n",
      "|     US|    536365|  2021-05-17|   500|            1300.0|\n",
      "|     IN|    536367|  2021-05-16|   600|             600.0|\n",
      "|     IN|    536367|  2021-05-20|   800|            1400.0|\n",
      "+-------+----------+------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"US\", \"536365\",\"2021-05-15\", \"600\"),\n",
    "             (\"US\", \"536365\",\"2021-05-17\", \"500\"),\n",
    "             (\"US\", \"536366\",\"2021-05-14\", \"200\"),\n",
    "             (\"IN\", \"536367\",\"2021-05-16\", \"600\"),\n",
    "             (\"IN\", \"536367\",\"2021-05-20\", \"800\")]\n",
    "\n",
    "df = spark.createDataFrame(data_list).toDF(\"Country\", \"CustomerID\", \"PurchaseDate\", \"Amount\")\n",
    "\n",
    "windowSpec = Window \\\n",
    ".partitionBy(\"Country\") \\\n",
    ".orderBy(\"PurchaseDate\") \\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df.withColumn(\"CumulativePurchase\", sum(\"Amount\").over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [(\"Germany\", \"48\", \"10\"),\n",
    "             (\"Germany\", \"49\", \"5\"),\n",
    "             (\"Germany\", \"50\", \"3\"),\n",
    "             (\"Germany\", \"51\", \"2\"),\n",
    "             (\"United Kingdom\", \"48\", \"2\"),\n",
    "             (\"United Kingdom\", \"49\", \"2\")]\n",
    "\n",
    "df = spark.createDataFrame(data_list).toDF(\"Country\", \"Week\", \"Quantity\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+----------+\n",
      "|       Country|Week|Quantity|3WeekTotal|\n",
      "+--------------+----+--------+----------+\n",
      "|       Germany|  48|      10|      10.0|\n",
      "|       Germany|  49|       5|      15.0|\n",
      "|       Germany|  50|       3|      18.0|\n",
      "|       Germany|  51|       2|      20.0|\n",
      "|United Kingdom|  48|       2|       2.0|\n",
      "|United Kingdom|  49|       2|       4.0|\n",
      "+--------------+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_total_window = Window.partitionBy(\"Country\").orderBy(\"Week\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df.withColumn(\"3WeekTotal\", sum(\"Quantity\").over(running_total_window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------+----------+\n",
      "|       Country|Week|Quantity|3WeekTotal|\n",
      "+--------------+----+--------+----------+\n",
      "|       Germany|  48|      10|      10.0|\n",
      "|       Germany|  49|       5|      15.0|\n",
      "|       Germany|  50|       3|      18.0|\n",
      "|       Germany|  51|       2|      10.0|\n",
      "|United Kingdom|  48|       2|       2.0|\n",
      "|United Kingdom|  49|       2|       4.0|\n",
      "+--------------+----+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_total_window = Window.partitionBy(\"Country\").orderBy(\"Week\").rowsBetween(-2, Window.currentRow)\n",
    "\n",
    "df.withColumn(\"3WeekTotal\", sum(\"Quantity\").over(running_total_window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------------+\n",
      "| Name|Department|          Score|\n",
      "+-----+----------+---------------+\n",
      "| Alma|        D0|          [100]|\n",
      "|Galma|        D1|[300, 250, 100]|\n",
      "|Salma|        D1|     [350, 100]|\n",
      "|Dalma|        D1|     [400, 100]|\n",
      "|Jalma|        D2|          [250]|\n",
      "|Nalma|        D2|[500, 300, 100]|\n",
      "|Lalma|        D3|     [300, 100]|\n",
      "+-----+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"Alma\",  \"D0\", [100]),\n",
    "             (\"Galma\", \"D1\", [300, 250, 100]),\n",
    "             (\"Salma\", \"D1\", [350, 100]),\n",
    "             (\"Dalma\", \"D1\", [400, 100]),\n",
    "             (\"Jalma\", \"D2\", [250]),\n",
    "             (\"Nalma\", \"D2\", [500, 300, 100]),\n",
    "             (\"Lalma\", \"D3\", [300, 100])]\n",
    "\n",
    "df = spark.createDataFrame(data_list).toDF(\"Name\", \"Department\", \"Score\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|Department| Name|Score|\n",
      "+----------+-----+-----+\n",
      "|        D0| Alma|  100|\n",
      "|        D1|Dalma|  400|\n",
      "|        D2|Nalma|  500|\n",
      "|        D3|Lalma|  300|\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "\n",
    "windowSpec = Window.partitionBy(\"Department\") \\\n",
    "    .orderBy(col(\"Score\").desc()) \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow) \n",
    "\n",
    "df.withColumn(\"Score\", explode(col(\"Score\"))) \\\n",
    "    .withColumn(\"rank\", dense_rank().over(windowSpec)) \\\n",
    "    .select(\"Department\", \"Name\", \"Score\") \\\n",
    "    .where(\"rank == 1\") \\\n",
    "    .orderBy(\"Department\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
