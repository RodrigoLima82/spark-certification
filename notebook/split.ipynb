{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-split\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "        ('Michael','Rose','','2000-05-19','M',4000),\n",
    "        ('Robert','','Williams','1978-09-05','M',4000),\n",
    "        ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "        ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Column using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year|month|day|\n",
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991|04   |01 |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |2000|05   |19 |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |1978|09   |05 |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |1967|12   |01 |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |1980|02   |17 |\n",
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n",
    "        .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n",
    "        .withColumn('day', split(df['dob'], '-').getItem(2))\n",
    "\n",
    "df1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year|month|day|\n",
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991|04   |01 |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |2000|05   |19 |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |1978|09   |05 |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |1967|12   |01 |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |1980|02   |17 |\n",
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, you can do like below by creating a function variable and reusing it.\n",
    "split_col = pyspark.sql.functions.split(df['dob'], '-')\n",
    "\n",
    "df2 = df.withColumn('year', split_col.getItem(0)) \\\n",
    "        .withColumn('month', split_col.getItem(1)) \\\n",
    "        .withColumn('day', split_col.getItem(2))\n",
    "\n",
    "df2.show(truncate=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Column using Select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+----+-----+---+\n",
      "|firstname|middlename|lastname|dob       |year|month|day|\n",
      "+---------+----------+--------+----------+----+-----+---+\n",
      "|James    |          |Smith   |1991-04-01|1991|04   |01 |\n",
      "|Michael  |Rose      |        |2000-05-19|2000|05   |19 |\n",
      "|Robert   |          |Williams|1978-09-05|1978|09   |05 |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|1967|12   |01 |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|1980|02   |17 |\n",
      "+---------+----------+--------+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_col = pyspark.sql.functions.split(df['dob'], '-')\n",
    "\n",
    "df3 = df.select(\"firstname\",\"middlename\",\"lastname\",\"dob\", \n",
    "                split_col.getItem(0).alias('year'),\n",
    "                split_col.getItem(1).alias('month'),\n",
    "                split_col.getItem(2).alias('day'))  \n",
    "                 \n",
    "df3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split with Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              str|\n",
      "+-----------------+\n",
      "|[one, two, three]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.createDataFrame([('oneAtwoBthree',)], ['str',])\n",
    "\n",
    "df4.select(split(df4.str, '[AB]').alias('str')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Using limit Argument\n",
    "\n",
    "Let’s see an example using limit option on split.\n",
    "\n",
    "limit > 0: The resulting array’s length will not be more than `limit`, and the resulting array’s last entry will contain all input beyond the last matched pattern.\n",
    "\n",
    "limit <= 0 will be applied as many times as possible, and the resulting array can be of any size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             str|\n",
      "+----------------+\n",
      "|[one, twoBthree]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.select(split(df4.str, '[AB]', 2).alias('str')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              str|\n",
      "+-----------------+\n",
      "|[one, two, three]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df4.select(split(df4.str, '[AB]',3).alias('str')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block displayed below contains an error. The code block below is intended to add a column itemNameElements to DataFrame itemsDf that includes an array of all words in column itemName. Find the error.\n",
    ">\n",
    "Sample of DataFrame itemsDf:\n",
    ">\n",
    "- +——————+—————————————————————————————————-+——————————————————-+\n",
    "- |itemId|itemName                          |supplier           |\n",
    "- +——————+—————————————————————————————————-+——————————————————-+\n",
    "- |1     |Thick Coat for Walking in the Snow|Sports Company Inc.|\n",
    "- |2     |Elegant Outdoors Summer Dress     |YetiX              |\n",
    "- |3     |Outdoors Backpack                 |Sports Company Inc.|\n",
    "- +——————+—————————————————————————————————-+——————————————————-+\n",
    ">\n",
    "Code block:\n",
    "\n",
    "- `itemsDf.withColumnRenamed(“itemNameElements”, split(“itemName”))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+--------------------+\n",
      "|itemId|            itemName|           supplier|    itemNameElements|\n",
      "+------+--------------------+-------------------+--------------------+\n",
      "|     1|Thick Coat for Wa...|Sports Company Inc.|[T, h, i, c, k,  ...|\n",
      "|     2|Elegant Outdoors ...|              YetiX|[E, l, e, g, a, n...|\n",
      "|     3|   Outdoors Backpack|Sports Company Inc.|[O, u, t, d, o, o...|\n",
      "+------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.withColumn(\"itemNameElements\", split(\"itemName\", \"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2 = df.randomSplit([0.70, 0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.text(\"../files/sample.txt\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+\n",
      "|              values|\n",
      "+--------------------+\n",
      "|[WHITE, HANGING, ...|\n",
      "|[, WHITE, METAL, ...|\n",
      "|[RED, WOOLLY, HOT...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(split(col(\"value\"), \" \").alias(\"values\"))\n",
    "df1.printSchema()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
