{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLODE() + SELECT() + SPLIT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question\n",
    "\n",
    "The code block displayed below contains an error. The code block should create DataFrame itemsAttributesDf which has columns itemId and attribute and lists every attribute from the attributes column in DataFrame itemsDf next to the itemId of the respective row in itemsDf. Find the error.\n",
    "\n",
    "A sample of DataFrame itemsDf is below.\n",
    ">\n",
    "\n",
    "- `+——————+—————————————————————————————+———————————————————+`\n",
    "- `|itemId|attributes                   |supplier           |`\n",
    "- `+——————+—————————————————————————————+———————————————————+`\n",
    "- `|1     |[blue, winter, cozy]         |Sports Company Inc.|`\n",
    "- `|2     |[red, summer, fresh, cooling]|YetiX              |`\n",
    "- `|3     |[green, summer, travel]      |Sports Company Inc.|`\n",
    "- `+——————+—————————————————————————————+———————————————————+`\n",
    "\n",
    "Code block:\n",
    "\n",
    "`itemsAttributesDf = itemsDf.explode(\"attributes\").alias(\"attribute\").select(\"attribute\", \"itemId\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-explode-select-split\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemId: string (nullable = true)\n",
      " |-- attributes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- supplier: string (nullable = true)\n",
      "\n",
      "+------+--------------------+-------------------+\n",
      "|itemId|          attributes|           supplier|\n",
      "+------+--------------------+-------------------+\n",
      "|     1|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|     2|[red, summer, fre...|              YetiX|\n",
      "|     3|[green, summer, t...|Sports Company Inc.|\n",
      "+------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        ('1',['blue', 'winter', 'cozy'],'Sports Company Inc.'),\n",
    "        ('2',['red', 'summer', 'fresh', 'cooling'],'YetiX'),\n",
    "        ('3',['green', 'summer', 'travel'],'Sports Company Inc.')]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema = ['itemId','attributes','supplier'])\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AttributeError: 'DataFrame' object has no attribute 'explode'\n",
    "itemsAttributesDf = itemsDf.explode(\"attributes\").alias(\"attribute\").select(\"attribute\", \"itemId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attribute: string (nullable = true)\n",
      " |-- itemId: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|attribute|itemId|\n",
      "+---------+------+\n",
      "|     blue|     1|\n",
      "|   winter|     1|\n",
      "|     cozy|     1|\n",
      "|      red|     2|\n",
      "|   summer|     2|\n",
      "|    fresh|     2|\n",
      "|  cooling|     2|\n",
      "|    green|     3|\n",
      "|   summer|     3|\n",
      "|   travel|     3|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsAttributesDf = itemsDf.select(explode(\"attributes\").alias(\"attribute\"),\"itemId\")\n",
    "itemsAttributesDf.printSchema()\n",
    "itemsAttributesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n",
      "|itemId|          attributes|           supplier|\n",
      "+------+--------------------+-------------------+\n",
      "|     1|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|     2|[red, summer, fre...|              YetiX|\n",
      "|     3|[green, summer, t...|Sports Company Inc.|\n",
      "+------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(NOT `attributes`)' due to data type mismatch: argument 1 requires boolean type, however, '`attributes`' is of array<string> type.;;\n'Project [NOT attributes#1 AS (NOT attributes)#53, NOT supplier#2 AS (NOT supplier)#54]\n+- LogicalRDD [itemId#0, attributes#1, supplier#2], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d158b0930b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The code block is intended to return all columns of DataFrame except for columns attributes and supplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mitemsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attributes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'supplier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(NOT `attributes`)' due to data type mismatch: argument 1 requires boolean type, however, '`attributes`' is of array<string> type.;;\n'Project [NOT attributes#1 AS (NOT attributes)#53, NOT supplier#2 AS (NOT supplier)#54]\n+- LogicalRDD [itemId#0, attributes#1, supplier#2], false\n"
     ]
    }
   ],
   "source": [
    "# AnalysisException: cannot resolve '(NOT `attributes`)' due to data type mismatch: argument 1 requires boolean type, however, '`attributes`' is of array<string> type.\n",
    "itemsDf.select(~col('attributes'), ~col('supplier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|itemId|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The code block is intended to return all columns of DataFrame except for columns attributes and supplier\n",
    "itemsDf.drop('attributes','supplier').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|attributes_exploded|\n",
      "+-------------------+\n",
      "|             winter|\n",
      "|            cooling|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(explode(\"attributes\").alias(\"attributes_exploded\")).filter(col(\"attributes_exploded\").contains(\"i\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'attributes_exploded' is not defined\n",
    "itemsDf.select(explode(\"attributes\").alias(\"attributes_exploded\")).filter(attributes_exploded.contains(\"i\"))\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'explode'\n",
    "itemsDf.explode(attributes).alias(\"attributes_exploded\").filter(col(\"attributes_exploded\").contains(\"i\"))\n",
    "\n",
    "# AttributeError: 'str' object has no attribute 'contains'\n",
    "itemsDf.select(explode(\"attributes\")).filter(\"attributes_exploded\".contains(\"i\"))\n",
    "\n",
    "# TypeError: 'Column' object is not callable\n",
    "itemsDf.select(col(\"attributes\").explode().alias(\"attributes_exploded\")).filter(col(\"attributes_exploded\").contains(\"i\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a single-column DataFrame showing the number of words in column supplier of DataFrame itemsDf?\n",
    ">\n",
    "- `itemsDf.split(\"supplier\", \" \").count()`\n",
    "- `itemsDf.split(\"supplier\", \" \").size()`\n",
    "- `itemsDf.select(word_count(\"supplier\"))`\n",
    "- `spark.select(size(split(col(supplier), \" \")))`\n",
    "- `itemsDf.select(size(split(\"supplier\", \" \")))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|size(split(supplier,  , -1))|\n",
      "+----------------------------+\n",
      "|                           3|\n",
      "|                           1|\n",
      "|                           3|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(size(split(\"supplier\", \" \"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 3, 4, 25, 1, None, 1587915332),\n",
    "         (2, 6, 7, 2, 2, None, 1586815312),\n",
    "         (3, 3, None, 25, 3, None, 1585824821),\n",
    "         (4, None, None, 3, 2, None, 1583244275),\n",
    "         (5, None, None, None, 2, None, 1575285427),\n",
    "         (6, 3, 2, 25, 2, None, 1572733275)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True),\n",
    "                     StructField('transactionDate', LongType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsDf.select('storeId', 'predError').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
