{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLODE() + SELECT() + SPLIT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question\n",
    "\n",
    "The code block displayed below contains an error. The code block should create DataFrame itemsAttributesDf which has columns itemId and attribute and lists every attribute from the attributes column in DataFrame itemsDf next to the itemId of the respective row in itemsDf. Find the error.\n",
    "\n",
    "A sample of DataFrame itemsDf is below.\n",
    ">\n",
    "\n",
    "- `+——————+—————————————————————————————+———————————————————+`\n",
    "- `|itemId|attributes                   |supplier           |`\n",
    "- `+——————+—————————————————————————————+———————————————————+`\n",
    "- `|1     |[blue, winter, cozy]         |Sports Company Inc.|`\n",
    "- `|2     |[red, summer, fresh, cooling]|YetiX              |`\n",
    "- `|3     |[green, summer, travel]      |Sports Company Inc.|`\n",
    "- `+——————+—————————————————————————————+———————————————————+`\n",
    "\n",
    "Code block:\n",
    "\n",
    "`itemsAttributesDf = itemsDf.explode(\"attributes\").alias(\"attribute\").select(\"attribute\", \"itemId\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"example-explode-select-split\")\n",
    "    .config('spark.serializer','org.apache.spark.serializer.KryoSerializer')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemId: string (nullable = true)\n",
      " |-- attributes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- supplier: string (nullable = true)\n",
      "\n",
      "+------+--------------------+-------------------+\n",
      "|itemId|          attributes|           supplier|\n",
      "+------+--------------------+-------------------+\n",
      "|     1|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|     2|[red, summer, fre...|              YetiX|\n",
      "|     3|[green, summer, t...|Sports Company Inc.|\n",
      "+------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        ('1',['blue', 'winter', 'cozy'],'Sports Company Inc.'),\n",
    "        ('2',['red', 'summer', 'fresh', 'cooling'],'YetiX'),\n",
    "        ('3',['green', 'summer', 'travel'],'Sports Company Inc.')]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema = ['itemId','attributes','supplier'])\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+--------------------+\n",
      "|itemId|          attributes|           supplier|                 now|\n",
      "+------+--------------------+-------------------+--------------------+\n",
      "|     1|[blue, winter, cozy]|Sports Company Inc.|2021-11-24 22:09:...|\n",
      "|     2|[red, summer, fre...|              YetiX|2021-11-24 22:09:...|\n",
      "|     3|[green, summer, t...|Sports Company Inc.|2021-11-24 22:09:...|\n",
      "+------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.withColumn(\"now\", current_timestamp()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|itemId|attributes|\n",
      "+------+----------+\n",
      "|     1|      blue|\n",
      "|     1|    winter|\n",
      "|     1|      cozy|\n",
      "|     2|       red|\n",
      "|     2|    summer|\n",
      "|     2|     fresh|\n",
      "|     2|   cooling|\n",
      "|     3|     green|\n",
      "|     3|    summer|\n",
      "|     3|    travel|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#itemsDf.selectExpr(\"itemId\", \"split(attributes) as attributes\").show()\n",
    "itemsDf.selectExpr(\"itemId\", \"explode(attributes) as attributes\").show()\n",
    "#itemsDf.select(\"itemId\", \"explode(attributes) as attributes\").show()\n",
    "#itemsDf.selectExpr(\"itemId\", \"splitArray(attributes) as attributes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'explode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-89ebfb16bf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# AttributeError: 'DataFrame' object has no attribute 'explode'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mitemsAttributesDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitemsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attributes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"itemId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \"\"\"\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'explode'"
     ]
    }
   ],
   "source": [
    "# AttributeError: 'DataFrame' object has no attribute 'explode'\n",
    "itemsAttributesDf = itemsDf.explode(\"attributes\").alias(\"attribute\").select(\"attribute\", \"itemId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attribute: string (nullable = true)\n",
      " |-- itemId: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|attribute|itemId|\n",
      "+---------+------+\n",
      "|     blue|     1|\n",
      "|   winter|     1|\n",
      "|     cozy|     1|\n",
      "|      red|     2|\n",
      "|   summer|     2|\n",
      "|    fresh|     2|\n",
      "|  cooling|     2|\n",
      "|    green|     3|\n",
      "|   summer|     3|\n",
      "|   travel|     3|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsAttributesDf = itemsDf.select(explode(\"attributes\").alias(\"attribute\"),\"itemId\")\n",
    "itemsAttributesDf.printSchema()\n",
    "itemsAttributesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n",
      "|itemId|          attributes|           supplier|\n",
      "+------+--------------------+-------------------+\n",
      "|     1|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|     2|[red, summer, fre...|              YetiX|\n",
      "|     3|[green, summer, t...|Sports Company Inc.|\n",
      "+------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(NOT attributes)' due to data type mismatch: argument 1 requires boolean type, however, 'attributes' is of array<string> type.;\n'Project [unresolvedalias(NOT attributes#1, Some(org.apache.spark.sql.Column$$Lambda$2628/947079569@1adf055d)), unresolvedalias(NOT supplier#2, Some(org.apache.spark.sql.Column$$Lambda$2628/947079569@1adf055d))]\n+- LogicalRDD [itemId#0, attributes#1, supplier#2], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1bd34b1757a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# AnalysisException: cannot resolve '(NOT `attributes`)' due to data type mismatch: argument 1 requires boolean type, however, '`attributes`' is of array<string> type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mitemsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attributes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'supplier'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '(NOT attributes)' due to data type mismatch: argument 1 requires boolean type, however, 'attributes' is of array<string> type.;\n'Project [unresolvedalias(NOT attributes#1, Some(org.apache.spark.sql.Column$$Lambda$2628/947079569@1adf055d)), unresolvedalias(NOT supplier#2, Some(org.apache.spark.sql.Column$$Lambda$2628/947079569@1adf055d))]\n+- LogicalRDD [itemId#0, attributes#1, supplier#2], false\n"
     ]
    }
   ],
   "source": [
    "# AnalysisException: cannot resolve '(NOT `attributes`)' due to data type mismatch: argument 1 requires boolean type, however, '`attributes`' is of array<string> type.\n",
    "itemsDf.select(~col('attributes'), ~col('supplier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|itemId|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The code block is intended to return all columns of DataFrame except for columns attributes and supplier\n",
    "itemsDf.drop('attributes','supplier').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|attributes_exploded|\n",
      "+-------------------+\n",
      "|             winter|\n",
      "|            cooling|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(explode(\"attributes\").alias(\"attributes_exploded\")).filter(col(\"attributes_exploded\").contains(\"i\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NameError: name 'attributes_exploded' is not defined\n",
    "itemsDf.select(explode(\"attributes\").alias(\"attributes_exploded\")).filter(attributes_exploded.contains(\"i\"))\n",
    "\n",
    "# AttributeError: 'DataFrame' object has no attribute 'explode'\n",
    "itemsDf.explode(attributes).alias(\"attributes_exploded\").filter(col(\"attributes_exploded\").contains(\"i\"))\n",
    "\n",
    "# AttributeError: 'str' object has no attribute 'contains'\n",
    "itemsDf.select(explode(\"attributes\")).filter(\"attributes_exploded\".contains(\"i\"))\n",
    "\n",
    "# TypeError: 'Column' object is not callable\n",
    "itemsDf.select(col(\"attributes\").explode().alias(\"attributes_exploded\")).filter(col(\"attributes_exploded\").contains(\"i\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a single-column DataFrame showing the number of words in column supplier of DataFrame itemsDf?\n",
    ">\n",
    "- `itemsDf.split(\"supplier\", \" \").count()`\n",
    "- `itemsDf.split(\"supplier\", \" \").size()`\n",
    "- `itemsDf.select(word_count(\"supplier\"))`\n",
    "- `spark.select(size(split(col(supplier), \" \")))`\n",
    "- `itemsDf.select(size(split(\"supplier\", \" \")))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|size(split(supplier,  , -1))|\n",
      "+----------------------------+\n",
      "|                           3|\n",
      "|                           1|\n",
      "|                           3|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(size(split(\"supplier\", \" \"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 3, 4, 25, 1, None, 1587915332),\n",
    "         (2, 6, 7, 2, 2, None, 1586815312),\n",
    "         (3, 3, None, 25, 3, None, 1585824821),\n",
    "         (4, None, None, 3, 2, None, 1583244275),\n",
    "         (5, None, None, None, 2, None, 1575285427),\n",
    "         (6, 3, 2, 25, 2, None, 1572733275)]\n",
    "\n",
    "schema = StructType([StructField('transactionId', IntegerType(), True),\n",
    "                     StructField('predError', IntegerType(), True),\n",
    "                     StructField('value', IntegerType(), True),\n",
    "                     StructField('storeId', IntegerType(), True),\n",
    "                     StructField('productId', IntegerType(), True),\n",
    "                     StructField('f', IntegerType(), True),\n",
    "                     StructField('transactionDate', LongType(), True)])\n",
    "\n",
    "transactionsDf = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(storeId=25, predError=3),\n",
       " Row(storeId=2, predError=6),\n",
       " Row(storeId=25, predError=3),\n",
       " Row(storeId=3, predError=None),\n",
       " Row(storeId=None, predError=None),\n",
       " Row(storeId=25, predError=3)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactionsDf.select('storeId', 'predError').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block shown below should return a DataFrame with columns transactionsId, predError, value, and f from DataFrame transactionsDf. Choose the answer that correctly fills the blanks in the code block to accomplish this.\n",
    ">\n",
    "- `transactionsDf.__1__(__2__)`\n",
    ">\n",
    "- `1. filter 2. \"transactionId\", \"predError\", \"value\", \"f\"`\n",
    "- `1. select 2. \"transactionId, predError, value, f\"`\n",
    "- `1. select 2. [\"transactionId\", \"predError\", \"value\", \"f\"]`\n",
    "- `1. where 2. col(\"transactionId\"), col(\"predError\"), col(\"value\"), col(\"f\")`\n",
    "- `1. select 2. col([\"transactionId\", \"predError\", \"value\", \"f\"])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-----+----+\n",
      "|transactionId|predError|value|   f|\n",
      "+-------------+---------+-----+----+\n",
      "|            1|        3|    4|null|\n",
      "|            2|        6|    7|null|\n",
      "|            3|        3| null|null|\n",
      "|            4|     null| null|null|\n",
      "|            5|     null| null|null|\n",
      "|            6|        3|    2|null|\n",
      "+-------------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactionsDf.select([\"transactionId\", \"predError\", \"value\", \"f\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block shown below should return a single-column DataFrame with a column named consonant_ct that, for each row, shows the number of consonants in column itemName of DataFrame itemsDf. Choose the answer that correctly fills the blanks in the code block to accomplish this.\n",
    ">\n",
    "- `itemsDf.select(__1__(__2__(__3__(__4__), \"a|e|i|o|u|s\", \"”)).__5__(consonant_ct”))`\n",
    ">\n",
    "- `1. length 2. regexp_extract 3. upper 4. col(\"itemName\") 5. as`\n",
    "- `1. size 2. regexp_replace 3. lower 4. \"itemName\" 5. alias`\n",
    "- `1. lower 2. regexp_replace 3. length 4. \"itemName\" 5. alias`\n",
    "- `1. length 2. regexp_replace 3. lower 4. col(\"itemName\") 5. alias`\n",
    "- `1. size 2. regexp_extract 3. lower 4. col(\"itemName\") 5. alias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- itemId: long (nullable = true)\n",
      " |-- itemName: string (nullable = true)\n",
      " |-- attributes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- supplier: string (nullable = true)\n",
      "\n",
      "+------+----------------------------------+--------------------+-------------------+\n",
      "|itemId|itemName                          |attributes          |supplier           |\n",
      "+------+----------------------------------+--------------------+-------------------+\n",
      "|1     |Thick Coat for Walking in the Snow|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|2     |Elegant Outdoors Summer Dress     |[red, summer]       |YetiX              |\n",
      "|3     |Outdoors Backpack                 |[green, summer]     |Sports Company Inc.|\n",
      "+------+----------------------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, 'Thick Coat for Walking in the Snow', ['blue', 'winter', 'cozy'], 'Sports Company Inc.'),\n",
    "        (2, 'Elegant Outdoors Summer Dress', ['red', 'summer'], 'YetiX'),\n",
    "        (3, 'Outdoors Backpack', ['green', 'summer'], 'Sports Company Inc.')]\n",
    "\n",
    "columns = [\"itemId\", \"itemName\", \"attributes\", \"supplier\"]\n",
    "\n",
    "itemsDf = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "itemsDf.printSchema()\n",
    "itemsDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|consonant_ct|\n",
      "+------------+\n",
      "|          34|\n",
      "|          29|\n",
      "|          17|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(lower(regexp_replace(length(\"itemName\"), \"a|e|i|o|u|s\", \"\")) \\\n",
    "       .alias(\"consonant_ct\")) \\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|consonant_ct|\n",
      "+------------+\n",
      "|          24|\n",
      "|          15|\n",
      "|          10|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(length(regexp_replace(lower(col(\"itemName\")), \"a|e|i|o|u|s\", \"\")) \\\n",
    "       .alias(\"consonant_ct\")) \\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y4JError: An error occurred while calling z:org.apache.spark.sql.functions.regexp_extract\n",
    "itemsDf.select(size(regexp_extract(lower(col(\"itemName\")), \"a|e|i|o|u|s\", \"\")).alias(\"consonant_ct\"))\n",
    "\n",
    "# SyntaxError: invalid syntax \"as\"\n",
    "itemsDf.select(length(regexp_extract(upper(col(\"itemName\")), \"a|e|i|o|u|s\", \"\")).as(\"consonant_ct\"))\n",
    "\n",
    "# AnalysisException: cannot resolve 'size(regexp_replace(lower(`itemName`), 'a|e|i|o|u|s', ''))' due to data type mismatch\n",
    "itemsDf.select(size(regexp_replace(lower(\"itemName\"), \"a|e|i|o|u|s\", \"\")).alias(\"consonant_ct\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+-------------------+\n",
      "|itemId|            itemName|          attributes|           supplier|\n",
      "+------+--------------------+--------------------+-------------------+\n",
      "|     1|Thick Coat for Wa...|[blue, winter, cozy]|Sports Company Inc.|\n",
      "|     2|Elegant Outdoors ...|       [red, summer]|              YetiX|\n",
      "|     3|   Outdoors Backpack|     [green, summer]|Sports Company Inc.|\n",
      "+------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|itemId|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.select(\"itemId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|itemId|increment|\n",
      "+------+---------+\n",
      "|     1|     0.20|\n",
      "|     2|     0.40|\n",
      "|     3|     0.00|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemsDf.selectExpr(\"itemId\", \"case when (itemId < 3) then itemId * 0.20 else 0 end as increment\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following Data Frame definition.\n",
    ">\n",
    "- `df = spark.range(500).toDF(\"number”)`\n",
    ">\n",
    "Choose the incorrect expression\n",
    ">\n",
    "- `df.select(\"number\" + 10)`\n",
    "- `df.select(df[\"number\"] + 10)`\n",
    "- `df.select(expr(\"number + 10\"))`\n",
    "- `df.select(col(\"number\") + 10)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(500).toDF('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df[\"number\"] + 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(expr(\"number + 10\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[(number + 10): bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col(\"number\") + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f968c826ad1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TypeError: can only concatenate str (not \"int\") to str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"number\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# TypeError: can only concatenate str (not \"int\") to str\n",
    "df.select(\"number\" + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|number|increment|\n",
      "+------+---------+\n",
      "|     0|     0.00|\n",
      "|     1|     0.20|\n",
      "|     2|     0.40|\n",
      "|     3|     0.60|\n",
      "|     4|     0.80|\n",
      "|     5|     1.00|\n",
      "|     6|     1.20|\n",
      "|     7|     1.40|\n",
      "|     8|     1.60|\n",
      "|     9|     1.80|\n",
      "|    10|     2.00|\n",
      "|    11|     2.20|\n",
      "|    12|     2.40|\n",
      "|    13|     2.60|\n",
      "|    14|     2.80|\n",
      "|    15|     3.00|\n",
      "|    16|     3.20|\n",
      "|    17|     3.40|\n",
      "|    18|     3.60|\n",
      "|    19|     3.80|\n",
      "+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df.selectExpr(\"number\", \"if(number < 20, number * 0.20 , 0) as increment\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following code blocks returns a DataFrame with a new column salary_increment and all previously existing columns.\n",
    ">\n",
    "- `df.withColumn(\"salary_increment\", \"salary * 0.15\")`\n",
    "- `df.select(\"*\", expr(\"salary * 0.15\").alias(\"salary_increment\"))`\n",
    "- `df.selectExpr(\"*\", expr(\"salary * 0.15\").alias(\"salary_increment\"))`\n",
    "- `All of the above`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "        ('Michael','Rose','','2000-05-19','M',4000),\n",
    "        ('Robert','','Williams','1978-09-05','M',4000),\n",
    "        ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "        ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"gender\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|salary_increment|\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|          450.00|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|          600.00|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|          600.00|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|          600.00|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           -0.15|\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", expr(\"salary * 0.15\").alias(\"salary_increment\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+---------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|(salary * 0.15)|\n",
      "+---------+----------+--------+----------+------+------+---------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|         450.00|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|         600.00|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|         600.00|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|         600.00|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|          -0.15|\n",
      "+---------+----------+--------+----------+------+------+---------------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|salary_increment|\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|          450.00|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|          600.00|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|          600.00|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|          600.00|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           -0.15|\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|salary_increment|\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|          450.00|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|          600.00|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|          600.00|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|          600.00|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           -0.15|\n",
      "+---------+----------+--------+----------+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\", \"salary * 0.15\").show()\n",
    "df.withColumn(\"salary_increment\", expr(\"salary * 0.15\")).show()\n",
    "df.selectExpr(\"*\", \"salary * 0.15 as salary_increment\").show()\n",
    "\n",
    "# TypeError: col should be Column\n",
    "# df.withColumn(\"salary_increment\", \"salary * 0.15\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AssertionError: col should be Column\n",
    "df.withColumn(\"salary_increment\", \"salary * 0.15\")\n",
    "\n",
    "# TypeError: Column is not iterable\n",
    "df.selectExpr(\"*\", expr(\"salary * 0.15\").alias(\"salary_increment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.withColumn(\"now\", now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [(\"Ravi\", \"28\", \"3200\"),\n",
    "             (\"Abdul\", \"23\", \"4800\"),\n",
    "             (\"John\", \"32\", \"6500\"),\n",
    "             (\"Rosy\", \"48\", \"8200\")]\n",
    "df = spark.createDataFrame(data_list).toDF(\"name\", \"age\", \"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", expr(\"salary\") * 0.20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", expr(\"salary * 0.20\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"name\", col(\"salary\") * 0.20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TypeError: can't multiply sequence by non-int of type 'float'\n",
    "df.select(\"name\", \"salary\" * 0.20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"avg(salary)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(expr(\"avg(salary)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnalysisException: cannot resolve '`avg(salary)`' given input columns: [age, name, salary]\n",
    "df.select(col(\"avg(salary)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac6ad6cad6e86b8f8ebaedbe94ebc31d728d6c0d8223a99e9449734cfa4d7995"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
